<!-- HTML header for doxygen 1.9.8-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
    <meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=11" />
    <meta name="generator" content="Doxygen 1.9.7" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Meltrix: Non-parametric Classifiers</title>
    <link href="tabs.css" rel="stylesheet" type="text/css" />
    <script type="text/javascript" src="jquery.js"></script>
    <script type="text/javascript" src="dynsections.js"></script> <link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
 <link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
 <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js"></script>
    <link href="doxygen.css" rel="stylesheet" type="text/css" /> <link href="doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-sidebar-only-darkmode-toggle.css" rel="stylesheet" type="text/css"/>
    <script type="text/javascript" src="doxygen-awesome-darkmode-toggle.js"></script>
    <script type="text/javascript">
        DoxygenAwesomeDarkModeToggle.init()
    </script>
    <script type="text/javascript" src="doxygen-awesome-interactive-toc.js"></script>
    <script type="text/javascript">
        DoxygenAwesomeInteractiveToc.init()
    </script>
</head>
<body>
    <div id="top">
        <!-- do not remove this div, it is closed by doxygen! -->
        <div id="titlearea">
            <table cellspacing="0" cellpadding="0">
                <tbody>
                <tr id="projectrow">
                    <td id="projectalign">
                        <div id="projectname">Meltrix
                            <span id="projectnumber">&#160;0.0.4</span>
                        </div>
                        <div id="projectbrief">A Simple Matrix Library Written in C++</div>
                    </td>
                </tr>
                </tbody>
            </table>
        </div>
        <!-- end header part --><!-- Generated by Doxygen 1.9.7 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('a01173.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Non-parametric Classifiers</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="autotoc_md6"></a>These classifiers use data directly at the classification time with no explicit model of the data, or otherwise put, it isn't governed by any parameters. There is also no explicit learning stage with this classifier. There is no real model of the classes when using non-parametric classifiers and data is not assumed to belong to any particular distribution. This means that it is more flexible than parametrised classification, but it can often be expensive and require a lot of data to learn things that were assumed by the parametric approach.</p>
<h1><a class="anchor" id="autotoc_md7"></a>
Nearest Neighbour</h1>
<p>This classification consists of labelling a new sample by copying the class of the nearest sample in a labelled database. There are no prior assumptions about the distributions of the instances in the database.</p>
<p>A very simple algorithm could be defined as so: </p><div class="fragment"><div class="line">d_min = infinity</div>
<div class="line">for i = 1 to N:</div>
<div class="line">     d = dist(y, x_i)</div>
<div class="line">     if d &lt; d_min:</div>
<div class="line">         ouput_label = w_i</div>
<div class="line">         d_min = d</div>
<div class="line">return output_label</div>
</div><!-- fragment --><p> Where in this algorithm <code>y</code> is the sample to classify, <code>x_i</code> is the training data, <code>w_i</code> are the labels and finally <code>dist()</code> is a predefined distance function.</p>
<p>This way of classification is very widely used in recognition problems including facial recognition, fingerprint verification and speech recognition, data mining with plagiarism detection and synonym detection, recommendation systems from music, films or shopping items and information retrieval in DNA sequences and related webpage search.</p>
<p>However, this classification has a couple of issues. The definition of similarity is very ambiguous, so a very rigid way of finding the similarities between two faces or two web pages have to be very thorough. The choice of similarity measure is also important as different measures may yield different results and finally, efficiently classifying new labels may prove difficult if a very large database is present of the similarity calculation isn't simple.</p>
<p>The decision boundary in a nearest neighbour classifier can be modelled using the Voronoi tessellation only if:</p><ul>
<li>$x_1,\cdots, x_N$ are all $L$-dimensional feature vectors</li>
<li>The nearest neighbour rule is used</li>
<li>There is a 2D distance measure in place <code>dist(x,y)</code></li>
</ul>
<p>When Voronoi tessellation is used, the feature vectors define a partition of the $L$-dimensional space into $N$ regions $R_i$: $$R_i=\lbrace x:d(x,x_i)&lt;d(x,x_j),i\neq j \rbrace$$ This means that $R_i$ contains all of the points in space that are closer to $x_i$ than any other points of the feature set. </p>
<h1><a class="anchor" id="autotoc_md8"></a>
$k$-Nearest Neighbours</h1>
<p>This is just a generalisation of the nearest neighbour classifier, however with $k$ neighbours and it takes the class of the majority in the $k$ neighbours it is compared to. There is still no training step, however the labelled data is sometimes referred to as the training set. This algorithm is very sensitive to the local structure of the database, meaning that it'll struggle with outliers and poorly struggled data.</p>
<p>In order to aid with the struggles, each neighbour can be weighted. One of the ways this can be used is weighting the neighbours such that the nearer the neighbour, the more it contributes on the thing to be classified. This could be written down mathematically as $\frac{1}{d}$ where $d$ is the distance from the sample to the neighbour.</p>
<p>The choice of $k$ also has a significant impact on the classification. Increasing $k$ reduces the effect of the noise, it makes the class boundaries smoother, however if $k$ is too large it can over-smooth the boundaries which may result in under-fitting. The optimum value of $k$ is data dependent and it can be sometimes selected by the use of some heuristics. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.7 </li>
  </ul>
</div>
</body>
</html>
